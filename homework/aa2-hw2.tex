\documentclass[11pt]{article}
\usepackage{master}
\DeclareMathOperator{\diam}{diam}
\newcommand{\interior}[1]{%
  {\kern0pt#1}^{\mathrm{o}}%
}

\title{Accelerated Analysis HW1}
\author{Andrew Hah}
\begin{document}

\pagestyle{plain}
\begin{center}
{\Large MATH 20410. Accelerated Analysis II Homework 2} \\ 
\vspace{.2in}  
Andrew Hah \\
Due January 22, 2025
\end{center}

\begin{exercise}{9.3}
    \begin{proof}
        Suppose $A(x_1) = A(x_2)$. Then $A(x_1 - x_2) = A(x_1) - A(x_2) = 0$. By assumption, $A(v) = 0 \iff v = 0$, therefore $x_1 - x_2 = 0 \implies x_1 = x_2$. Thus $A$ is 1-1. 
    \end{proof}
\end{exercise}

\begin{exercise}{9.5}
    \begin{proof}
        Let $e_1, \dots, e_n$ be the standard basis of $\mathbb{R}^n$. Define $y \in \bbR^n$ by $y_i = A(e_i)$ for $i = 1, \dots, n$. Then for any $x = (x_1, \dots, x_n)$, we have $$A(x) = A \left( \sum_{i = 1}^n x_i e_i \right) = \sum_{i = 1}^n x_i A(e_i) = \sum_{i = 1}^n x_i y_i = x \cdot y$$ This shows the existence of $y$. To see uniqueness, suppose $y_1$ and $y_2$ both, for all $x$, give $x \cdot y_1 = x \cdot y_2$. Then their difference defines the linear map $B(x) = x \cdot (y_1 - y_2)$. This is zero everywhere, since $x \cdot y_1 - x \cdot y_2 = 0$ for all $x$. Thus $y_1 - y_2$ must be the zero vector, since if $y_1 - y_2 \neq 0$, taking $x = y_1 - y_2$ would give $B(y_1 - y_2) = (y_1 - y_2) \cdot (y_1 - y_2) = \| y_1 - y_2 \|^2 \neq 0$, contradicting that $B$ is zero everywhere. Hence $y_1 - y_2 = 0 \implies y_1 = y_2$. \\

        Since $A(x) = x \cdot y$, we have \begin{equation*}
                \| A \|  = \sup_{x \neq 0} \frac{| A(x) |}{\| x \|} = \sup_{\| x \| = 1} |x \cdot y| 
        \end{equation*} By the Cauchy-Schwarz inequality, for every $x$ with $\| x \| = 1$, $| x \cdot y | \le \| x \| \| y \| = \| y \|$ hence $\| A \| \le \| y \|$. To see that we get equality, take $x = \frac{y}{\| y \|}$. Then $$A(x) = x \cdot y = \left( \frac{y}{\| y \|} \right) \cdot y = \| y \|$$ so indeed $\| A \| \ge \| y \|$.         
    \end{proof}
\end{exercise}

\begin{exercise}{1}
    \begin{proof}
        The $(2n - 1)$-th degree Taylor polynomial of $f$ at $c$ is $$P_{2n - 1} (x) = \sum_{j = 0}^{2n - 1} \frac{f^{(j)}(c)}{j!} (x - c)^j$$ Since all derivatives up to order $2n - 1$ are 0, this simplifies to $P_{2n - 1}(x) = f(c)$. By Taylor's Theorem, for each $x \neq c$, there exists $\lambda$ between $x$ and $c$ such that $f(x) = f(c) + \frac{f^{(2n)}(\lambda)}{(2n)!}(x - c)^{2n}$. Because $f^{(2n)}(\lambda) > 0$ and $(x - c)^{2n} \ge 0$, for $x$ near $c$ if $x \neq c$, then $$f(x) - f(c) = \frac{f^{(2n)}(\lambda)}{(2n)!} (x - c)^{2n} > 0$$ so $f(x) > f(c)$. Therefore $c$ is indeed a local minimum of $f$. 
    \end{proof}
\end{exercise}

\begin{exercise}{2}
    \begin{enumerate} [(a)]
        \item \begin{proof}
            We observe how $\phi$ acts on the standard basis of $\bbR^2$. Let $e_1 = (1, 0)$ and $e_2 = (0, 1)$. Then $\phi(e_1) = (1, 2, 0)$ and $\phi(e_2) = (2, -1, 3)$. Thus the columns of matrix $[\phi]$ are exactly these images, i.e. \begin{equation*}
            [\phi] = 
            \begin{pmatrix}
                1 & 2 \\
                2 & -1 \\
                0 & 3 \\
            \end{pmatrix}
        \end{equation*}
        To find $\| \phi \| $, we know $\| \phi \| = \sup_{\| v = 1 \|} \| \phi(v) \|$, so we maximize $\| \phi(v) \| $ over all unit vectors $v = (x, y)$ such that $x^2 + y^2 = 1$. We see that $$\| \phi(v) \| = \sqrt{(x + 2y)^2 + (2x - y)^2 + (3y)^2}$$ Simplifying, we get \begin{equation*}
            \begin{split}
                \| \phi(v) \|^2 & = (x + 2y)^2 + (2x - y)^2 + (3y)^2 \\
                & = x^2 + 4xy + 4y^2 + 4x^2 - 4xy + y^2 + 9y^2 \\
                & = 5x^2 + 14y^2
            \end{split}
        \end{equation*}
        Maximizing this, we get \begin{equation*}
            \begin{split}
                \| \phi(v) \|^2 & = 5x^2 + 14(1 - x^2) \\ 
                & = 14 - 9x^2
            \end{split}
        \end{equation*}
        Thus $x = 0$, and we get $\| \phi(v) \|^2 = 14 \implies \| \phi \| = \sqrt{14}$. 
        \end{proof}
        \item \begin{proof} The corresponding linear map $T: \bbR^3 \to \bbR^3$ in the standard basis is
            \begin{equation*}
                T(x, y, z) = \begin{pmatrix}
                    1 & -2 & 3 \\
                    0 & -5 & 1 \\
                \end{pmatrix} \begin{pmatrix}
                    x \\
                    y \\
                    z \\
                \end{pmatrix} = (x - 2y+3z, -5y + z)
            \end{equation*}
        \end{proof}
    \end{enumerate}
\end{exercise}

\begin{exercise}{3}
    \begin{proof}
        We see that $\frac{\| Ah \|}{\| h \|}$ is constant if we scale $h$. Fix any nonzero $x \in \bbR^n$ and consider $h = tx$. Then for $t \neq 0$, $$\frac{\| A(tx) \| }{\| tx \| } = \frac{|t| \| Ax \| }{|t| \| x \| } = \frac{\| Ax \| }{\| x \| }$$ Because $\lim_{h \to 0} \frac{\| Ah \| }{\| h \| } = 0$, we can let $t \to 0$, while the ratio stays equal to $\frac{\| Ax \| }{\| x\| }$. The only way for this limit to be zero is that $\| Ax \| = 0$. Since $x$ was arbitrary, $A$ is the zero transformation. 
    \end{proof}
\end{exercise}

\begin{exercise}{4}
    \begin{proof}
        Note that the function $x \mapsto \| Ax \| $ is continuous, and we are looking at it on the compact set $\{ x : \| x \| = 1 \}$. We know that a continuous function on a compact set achieves its maximum, i.e. $\exists x$ s.t. $$\| Ax \| = \max_{\| u \| = 1} \| Au\| = \| A \|$$ Hence we get an $x \in \bbR^n$ with $\| x \| = 1$ s.t. $\|Ax \| = \| A \| $. 
    \end{proof}
\end{exercise}

\begin{exercise}{5}
    \begin{enumerate} [(a)]
        \item \begin{proof}
            First, to show $\pi_I$ is linear, let $x=(x_1,\ldots,x_n)$ and $y=(y_1,\ldots,y_n)$ be in $\mathbb{R}^n$, and let $\alpha,\beta$ be scalars. Then
\[
  \pi_I(\alpha x + \beta y)
  \;=\;\bigl((\alpha x_1 + \beta y_1)_{i_1}, \dots, (\alpha x_n + \beta y_n)_{i_k}\bigr)
  \;=\;\alpha\,(x_{i_1},\dots,x_{i_k}) + \beta\,(y_{i_1},\dots,y_{i_k})
  \;=\;\alpha\,\pi_I(x)\;+\;\beta\,\pi_I(y).
\]
Hence $\pi_I$ is linear.

        Next, to find $\|\pi_I\|$, we note
\[
  \|\pi_I(x)\|^2
    \;=\;\sum_{j=1}^k x_{\,i_j}^2
    \;\le\;\sum_{r=1}^n x_r^2
    \;=\;\|x\|^2.
\]
Thus $\|\pi_I(x)\|\le\|x\|$ for all $x$, implying $\|\pi_I\|\le1$.  

On the other hand, if you pick $x$ that is nonzero only in the coordinates $i_j$ (for instance, take $x_{i_j}=1$ and all other coordinates $0$), then $\|\pi_I x\|=\|x\|$.  This forces $\|\pi_I\|\ge1$.  Combining these shows
\[
  \|\pi_I\|\;=\;1.
\]

Finally, the matrix representation of $\pi_I$ in the standard bases is the $k\times n$ matrix whose $j$-th row has a $1$ in the column $i_j$ and $0$ elsewhere.  Visually,
\[
 [\pi_I]
   \;=\;
   \begin{pmatrix}
     0 & \cdots & 0 & 1 & 0 & \cdots & 0 & \cdots & 0\\
     \vdots &&&&&&&&\vdots\\
     0 & \cdots & 0 & 0 & 0 & \cdots & 1 & \cdots & 0
   \end{pmatrix},
\]
where the $j$-th row picks out the $(i_j)$-th coordinate of the input vector in $\mathbb{R}^n$.
        \end{proof}
        \item \begin{proof}

        $(\implies)$ If $f$ is continuous at $x$, then given any $\epsilon>0$, there is some $\delta>0$ such that for all $y\in A$ with $\|y-x\|<\delta$, we have $\|f(y)-f(x)\|<\epsilon$.  In particular,
\[
 |f_i(y) - f_i(x)|
  \;\le\;\|f(y)-f(x)\|
  \;<\;\epsilon
\]
since the distance $\|u\|$ in $\mathbb{R}^m$ controls every coordinate difference $|u_i|$.  
Hence each $f_i$ is continuous at $x$.

    $(\impliedby)$ Suppose each $f_i$ is continuous at $x$.  Then for each $i$ and for each $\epsilon>0$, there is a $\delta_i>0$ such that
\[
 \|\,y - x\|\;<\;\delta_i
   \quad\Longrightarrow\quad
 |f_i(y) - f_i(x)| \;<\;\frac{\epsilon}{\sqrt{m}}.
\]
Let $\delta=\min\{\delta_1,\dots,\delta_m\}$.  If $\|y-x\|<\delta$, then \emph{all} coordinates satisfy $|f_i(y)-f_i(x)|<\tfrac{\epsilon}{\sqrt{m}}$.  Consequently,
\[
 \|f(y)-f(x)\|^2
   \;=\;\sum_{i=1}^m \bigl(f_i(y)-f_i(x)\bigr)^2
   \;<\;m \left(\frac{\epsilon}{\sqrt{m}}\right)^2
   \;=\;\epsilon^2,
\]
i.e.\ $\|f(y)-f(x)\|<\epsilon$.  Therefore $f$ is continuous at $x$.  

Thus $f$ is continuous at $x$ if and only if all $f_i$ are continuous at $x$.
        \end{proof}
    \end{enumerate}
\end{exercise}

\begin{exercise}{6}
    \begin{enumerate} [(a)]
        \item \begin{proof}
            By Cauchy--Schwarz,
\[
 \|Ax\|^2
   \;=\;\sum_{i=1}^m \left(\sum_{j=1}^n a_{ij}\,x_j\right)^2
   \;\le\;\sum_{i=1}^m \left(\sqrt{\sum_{j=1}^n a_{ij}^2}\;\|x\|\right)^2
   \;=\;\left(\sum_{i,j} a_{ij}^2\right)\,\|x\|^2
   \;=\;\|A\|_F^2 \,\|x\|^2.
\]
Thus $\|Ax\|\le \|A\|_F\,\|x\|$ for all $x$, and taking $\sup_{\|x\|=1}$ gives
\[
  \|A\|_{\mathrm{op}}
    \;\le\;\|A\|_F.
\]

For the reverse inequality,
write $A$ as $\bigl[v_1\mid v_2\mid\cdots\mid v_n\bigr]$ where each $v_j\in \mathbb{R}^m$
is a column.  Note that $v_j = A\,e_j$ if $e_j$ is the $j$-th standard basis vector in
$\mathbb{R}^n$. Hence
\[
  \|v_j\|
    \;=\;\|A\,e_j\|
    \;\le\;\|A\|_{\mathrm{op}}\;\|e_j\|
    \;=\;\|A\|_{\mathrm{op}}.
\]
Also,
\[
  \|v_j\|^2
    \;=\;\sum_{i=1}^m a_{ij}^2
    \;\;\text{(the sum of squares in column }j\text{).}
\]
Thus
\[
  \sum_{j=1}^n\sum_{i=1}^m a_{ij}^2
    \;=\;\sum_{j=1}^n \|v_j\|^2
    \;\le\;\sum_{j=1}^n \|A\|_{\mathrm{op}}^2
    \;=\;n\;\|A\|_{\mathrm{op}}^2.
\]
Therefore,
\[
  \|A\|_{F}^2
    \;=\;\sum_{i=1}^m\sum_{j=1}^n a_{ij}^2
    \;\le\;n\;\|A\|_{\mathrm{op}}^2
    \quad\Longrightarrow\quad
  \|A\|_{F}\;\le\;\sqrt{n}\,\|A\|_{\mathrm{op}}.
\]
If $m \le n$, this already implies
$\|A\|_{F}\le \sqrt{\min(m,n)}\,\|A\|_{\mathrm{op}}$.

If $m>n$, we do a similar bound by rows.  Denote by $\rho_i$ the $i$-th row of $A$.  We can
think of $A^\mathsf{T}$ as an $n\times m$ matrix whose $i$-th column is $\rho_i$.
We have
\[
  \|\rho_i\|
    \;=\;\| (A^\mathsf{T})\,e_i'\|
    \;\le\;\|A^\mathsf{T}\|_{\mathrm{op}}
    \;=\;\|A\|_{\mathrm{op}},
\]
where $e_i'$ is the $i$-th standard basis vector in $\mathbb{R}^m$. Also
\[
  \|\rho_i\|^2
    \;=\;\sum_{j=1}^n a_{ij}^2.
\]
Summing over $i=1,\dots,m$ gives
\[
  \|A\|_F^2
    \;=\;\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2
    \;=\;\sum_{i=1}^m \|\rho_i\|^2
    \;\le\;m\,\|A\|_{\mathrm{op}}^2.
\]
Hence
\[
  \|A\|_F \;\le\;\sqrt{m}\,\|A\|_{\mathrm{op}}.
\]

Combining the two inequalities,
\[
  \|A\|_{F}
    \;\le\;\sqrt{n}\,\|A\|_{\mathrm{op}}
  \quad\text{and}\quad
  \|A\|_{F}
    \;\le\;\sqrt{m}\,\|A\|_{\mathrm{op}}
\]
immediately implies
\[
  \|A\|_{F}
    \;\le\;\sqrt{\min(m,n)}\;\|A\|_{\mathrm{op}}.
\]
Setting $c = 1/\sqrt{\min(m,n)}$ and $C=1$ yields the required two‐sided bound.
        \end{proof}
        \item \begin{proof}
            Because of part (a), for every $A,B\in L(\mathbb{R}^n,\mathbb{R}^m)$ we have
\[
  \|A-B\|_{\mathrm{op}}
    \;\le\;
  C\,\|A-B\|_F
  \qquad\text{and}\qquad
  \|A-B\|_F
    \;\le\;
  \frac{1}{c}\,\|A-B\|_{\mathrm{op}}.
\]
Hence the two distances 
\[
  d_{\mathrm{op}}(A,B):=\|A-B\|_{\mathrm{op}},
  \quad
  d_F(A,B):=\|A-B\|_F
\]
control one another up to a multiplicative constant.  This implies precisely that a sequence $A_n$ converges to $A$ in $d_{\mathrm{op}}$ if and only if it converges to $A$ in $d_F$.  
Equivalently, the $\epsilon$‐balls in one metric contain an $\epsilon'$‐ball in the other, so open sets (and closed sets) for $d_{\mathrm{op}}$ are exactly open (and closed) for $d_F$.  
Thus they generate the same topology on $L(\mathbb{R}^n,\mathbb{R}^m)$.
        \end{proof}
    \end{enumerate}
\end{exercise}

\end{document}