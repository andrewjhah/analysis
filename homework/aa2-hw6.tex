\documentclass[11pt]{article}
\usepackage{master}
\DeclareMathOperator{\diam}{diam}
\newcommand{\interior}[1]{%
  {\kern0pt#1}^{\mathrm{o}}%
}

\title{Accelerated Analysis HW6}
\author{Andrew Hah}
\begin{document}

\pagestyle{plain}
\begin{center}
{\Large MATH 20410. Accelerated Analysis II Homework 6} \\ 
\vspace{.2in}  
Andrew Hah \\
Due February 19, 2025
\end{center}

\begin{exercise}{6.1}
    \begin{proof} Analyzing $\sup f$ and $\inf f$ on subintervals:
\begin{enumerate} [(i)]
\item If a subinterval $[x_{i-1},x_i]$ does not contain $x_0$, then $f(x)=0$ for all $x$ in that subinterval. Hence
\[
\sup_{x\in[x_{i-1},x_i]} f(x)
\;=\;
0,
\quad
\inf_{x\in[x_{i-1},x_i]} f(x)
\;=\;
0.
\]
\item If a subinterval $[x_{i-1},x_i]$ does contain $x_0$, then 
\[
\sup_{x\in[x_{i-1},x_i]} f(x)
\;=\;
1,
\quad
\inf_{x\in[x_{i-1},x_i]} f(x)
\;=\;
0.
\]
\end{enumerate}

Thus, on any partition $P$, exactly one subinterval (the one containing $x_0$) will have $\sup f = 1$ and $\inf f = 0$; all other intervals will have $\sup f = \inf f = 0$.  Consequently:
\[
U(f,\alpha,P)
= \sum_{i=1}^n 
\sup f|_{[x_{i-1},x_i]}\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr]
= \bigl[\alpha(x_j) - \alpha(x_{j-1})\bigr]
\]
where $x_{j-1} < x_0 < x_j$, and
\[
L(f,\alpha,P)
= \sum_{i=1}^n 
\inf f|_{[x_{i-1},x_i]}\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr]
= 0.
\]
Hence
\[
U(f,\alpha,P) - L(f,\alpha,P)
\;=\;
\alpha(x_j) - \alpha(x_{j-1}).
\]

We want to make $\alpha(x_j) - \alpha(x_{j-1})$ arbitrarily small by a suitable partition $P$.  Because $\alpha$ is continuous at $x_0$, for every $\eta>0$ there exists $\delta>0$ such that if $|x - x_0| < \delta$, then $|\alpha(x) - \alpha(x_0)| < \eta$.  

Choose $\eta = \epsilon / 2$.  Then let us pick a partition $P$ such that $x_0$ lies in some subinterval $[x_{j-1},x_j]$ where $x_j-x_{j-1}<\delta$.  Because $\alpha$ is increasing and continuous near $x_0$, we have
\[
\alpha(x_j) - \alpha(x_{j-1})
\;\le\;
|\alpha(x_j) - \alpha(x_0)| 
\;+\;
|\alpha(x_0) - \alpha(x_{j-1})|
\;<\;
\eta + \eta
\;=\;
2\eta.
\]
Therefore,
\[
U(f,\alpha,P) - L(f,\alpha,P)
\;=\;
\alpha(x_j) - \alpha(x_{j-1})
\;<\;
\varepsilon.
\]
This shows that for every $\varepsilon>0$, we can find a partition making the difference of upper and lower sums less than $\varepsilon$.  By definition, $f\in R(\alpha)$.

We claim that $\int_a^b f\,d\alpha = 0$.  By the Riemann--Stieltjes integrability of $f$, for every $\varepsilon>0$ we can pick a partition $P$ such that \emph{all} Riemann--Stieltjes sums $S(f,\alpha;Q,\{t_i\})$ (for any refinement $Q$ of $P$ and any choice of sample points $\{t_i\}$) lie within $\varepsilon$ of $\int_a^b f\,d\alpha$.  

But look at the Riemann--Stieltjes sum:
\[
S(f,\alpha;P,\{t_i\})
= \sum_{i=1}^n f(t_i)\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr].
\]
Since $f(t_i) = 0$ unless $t_i = x_0$, and $x_0$ can lie in at most one subinterval $[x_{i-1},x_i]$, the sum has at most one nonzero term.  Moreover, if $t_i=x_0$ happens to be chosen, then 
\[
f(t_i)\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr]
= 1\cdot\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr].
\]
But from before, we can make $\alpha(x_i)-\alpha(x_{i-1})$ as small as we wish by ensuring $x_0$ is in a very small subinterval.  Hence all Riemann--Stieltjes sums can be made arbitrarily close to $0$.  Therefore, the integral $\int_a^b f\,d\alpha$ must be $0$.
\end{proof}
\end{exercise}

\begin{exercise}{6.2}
    \begin{proof}
Assume, for the sake of contradiction, that $f$ is not identically zero.  
Then there exists some $c\in[a,b]$ with $f(c) > 0$.  
By continuity of $f$ at $c$, there exists $\delta>0$ such that for all $x$ in the interval 
\[
I \;=\; (c-\delta,c+\delta)\,\cap\,[a,b],
\]
we have $f(x) > \tfrac12\,f(c) > 0$.  

Hence,
\[
\int_a^b f(x)\,dx 
\;\ge\; 
\int_{I} f(x)\,dx 
\;>\;
\int_{I} \tfrac12\,f(c)\,dx 
\;=\;
\tfrac12\,f(c)\,\bigl(\max I - \min I\bigr)
\;>\;0,
\]
contradicting the hypothesis that $\int_a^b f(x)\,dx=0$.  
Therefore, our assumption must be false, and $f$ must be identically zero on $[a,b]$.
\end{proof}
\end{exercise}

\begin{exercise}{1}
    \begin{proof} Since \(D g(x_0,y_0)\neq (0,0)\), at least one of the partial derivatives of \(g\) at \((x_0,y_0)\) is nonzero.  Without loss of generality, assume
\(
\tfrac{\partial g}{\partial y}(x_0,y_0)\;\neq\;0.
\)
By the Implicit Function Theorem, there is a neighborhood \(U\) of \(x_0\) and a differentiable function \(h:U \to \mathbb{R}\) such that for \(x\) in \(U\),
\[
g\bigl(x,h(x)\bigr)\;=\;0
\quad\text{and}\quad
h(x_0)\;=\;y_0.
\]
In other words, near \((x_0,y_0)\), every \((x,y)\) satisfying \(g(x,y)=0\) can be written as \(\bigl(x,h(x)\bigr)\) for \(x\) close to \(x_0\). Define the single-variable function
\(
F(x)\;=\;f\bigl(x,h(x)\bigr).
\)
Then \(F\) is continuously differentiable for \(x\) near \(x_0\).  Because \((x_0,y_0)\) is a local maximum of \(f\) on the set \(S\), and all points of \(S\) near \((x_0,y_0)\) look like \(\bigl(x,h(x)\bigr)\), it follows that \(x_0\) is a local maximum of \(F\).  Concretely:
\[
f\bigl(x_0,h(x_0)\bigr) \;=\; f(x_0,y_0)
\quad\text{is at least as large as}\quad
f\bigl(x,h(x)\bigr)
\quad\text{for }x\text{ near }x_0.
\]
Hence \(F\) has a local maximum at \(x_0\). Since \(F\) has a local maximum at \(x_0\), we must have \(F'(x_0)=0\).  By the chain rule,
\[
F'(x_0)
\;=\;
\tfrac{d}{dx}\,f\bigl(x,h(x)\bigr)\Bigl|_{x=x_0}
\;=\;
\tfrac{\partial f}{\partial x}(x_0,y_0)
\;+\;
\tfrac{\partial f}{\partial y}(x_0,y_0)\,\tfrac{d}{dx}\bigl(h(x)\bigr)\Bigl|_{x=x_0}.
\]
Let us denote
\[
f_x \;=\;\tfrac{\partial f}{\partial x}(x_0,y_0),
\quad
f_y \;=\;\tfrac{\partial f}{\partial y}(x_0,y_0),
\quad
h'(x_0) \;=\;\tfrac{d}{dx}\bigl(h(x)\bigr)\Bigl|_{x=x_0}.
\]
Thus
\[
F'(x_0) 
\;=\;
f_x + f_y\,h'(x_0)
\;=\;0.
\]
So we obtain:
\[
f_x + f_y\,h'(x_0) \;=\;0.
\;\;\;\;(1)
\]

Since \(g\bigl(x,h(x)\bigr)=0\) for all \(x\) near \(x_0\), its derivative at \(x_0\) is also zero.  Again by the chain rule:
\[
0
\;=\;
\tfrac{d}{dx}\,g\bigl(x,h(x)\bigr)\Bigl|_{x=x_0}
\;=\;
\tfrac{\partial g}{\partial x}(x_0,y_0)
\;+\;
\tfrac{\partial g}{\partial y}(x_0,y_0)\,\tfrac{d}{dx}\bigl(h(x)\bigr)\Bigl|_{x=x_0}.
\]
Let us set
\[
g_x \;=\;\tfrac{\partial g}{\partial x}(x_0,y_0),
\quad
g_y \;=\;\tfrac{\partial g}{\partial y}(x_0,y_0).
\]
Then
\[
0
\;=\;
g_x + g_y\,h'(x_0).
\;\;\;\;(2)
\]

From equations \((1)\) and \((2)\), we have the linear system
\[
\begin{cases}
f_x + f_y\,h'(x_0)\;=\;0, \\
g_x + g_y\,h'(x_0)\;=\;0.
\end{cases}
\]
We can view this system as
\[
\begin{pmatrix}f_x \\ g_x\end{pmatrix}
\;+\;
\begin{pmatrix}f_y \\ g_y\end{pmatrix}\,h'(x_0)
\;=\;
\begin{pmatrix}0 \\ 0\end{pmatrix}.
\]
Since \(g_y \neq 0\) by assumption, the second equation implies
\[
h'(x_0) \;=\; -\frac{g_x}{g_y}.
\]
Substituting back into the first equation, we can rearrange to see that
\[
\frac{f_x}{g_x}
\;=\;
\frac{f_y}{g_y},
\]
provided \(g_x\) and \(g_y\) are not both zero (which we know they are not, because \(D g(x_0,y_0)\neq (0,0)\)).  Hence there exists some real number \(\lambda\) such that
\[
f_x \;=\;\lambda\,g_x,
\quad
f_y \;=\;\lambda\,g_y.
\]
In other words,
\[
D f(x_0,y_0)
\;=\;
\lambda\;D g(x_0,y_0).
\]
This is exactly the conclusion we needed.
\end{proof}
\end{exercise}

\begin{exercise}{2}
    \begin{proof} Since 
\[
\frac{\partial F}{\partial y}(x_0,y_0)\;\neq\;0,
\]
the Implicit Function Theorem tells us that there is an open interval $I$ containing $x_0$ and a $C^1$ function
\(
g: I \;\to\; \mathbb{R}
\)
such that:
\[
g(x_0) \;=\; y_0,
\quad\text{and}\quad
F\bigl(x,\;g(x)\bigr) \;=\; 0 \quad \text{for all }x \in I.
\]

We now introduce new coordinates:
\(
t = y - g(x).
\)
In other words, for each fixed $x$, we measure how far $y$ is from the ``implicit'' solution $g(x)$.  Concretely, define
\[
G\colon I \times \mathbb{R} \;\to\; \mathbb{R}
\quad\text{by}\quad
G\bigl(x,t\bigr) 
\;=\; 
F\bigl(x,\;g(x) \;+\; t\bigr).
\]
Because $F$ is $C^1$ and $g$ is $C^1$, the composition $G(x,t)$ is at least continuous. Let $U$ be a suitable open set of $(x_0,y_0)$ such that for each $(x,y)\in U$, we have $x\in I$ and $y$ is close to $g(x)$.  In that region, define
\(
t = y - g(x).
\)
Then by construction,
\[
F(x,y) 
\;=\; 
F\bigl(x,\;g(x) + t\bigr) 
\;=\; 
G\bigl(x,\;t\bigr) 
\;=\;
G\bigl(x,\;y - g(x)\bigr).
\]
Hence for all $(x,y)$ in this neighborhood $U$, the identity
\[
F(x,y) 
\;=\; 
G\bigl(x,\;y - g(x)\bigr)
\]
holds.  
We have found:
\begin{enumerate} [(i)]
\item a $C^1$ function $g(x)$ on an interval $I$ around $x_0$,
\item a continuous function $G(x,t)$ on an open set $I\times J\subset\mathbb{R}^2$ containing $(x_0,0)$,
\item an open neighborhood $U\subset \mathbb{R}^2$ of $(x_0,y_0)$ (where $y_0=g(x_0)$),
\end{enumerate}
such that the desired re-expression
\[
F(x,y) 
\;=\; 
G\bigl(x,\;y - g(x)\bigr)
\]
holds for all $(x,y)\in U$. 
\end{proof}
\end{exercise}

\begin{exercise}{3}
    \begin{proof} (i) $\iff$ (ii). By definition, $\alpha\in R(\beta)$ means that for every $\epsilon>0$ there exists a partition $P$ such that
\[
U(\alpha,\beta,P)-L(\alpha,\beta,P)<\epsilon.
\]
In other words, for every $\epsilon>0$ we may choose $P$ so that
\[
\sum_{i=1}^n\Bigl[\alpha(x_i)-\alpha(x_{i-1})\Bigr]\Bigl[\beta(x_i)-\beta(x_{i-1})\Bigr]<\epsilon.
\tag{1}
\]

Now, define the sum for $\alpha$ with respect to $\beta$ using the right endpoints:
\[
S_R(\alpha,\beta;P)=\sum_{i=1}^n \alpha(x_i)\,[\beta(x_i)-\beta(x_{i-1})],
\]
and the sum for $\beta$ with respect to $\alpha$ using the left endpoints:
\[
S_L(\beta,\alpha;P)=\sum_{i=1}^n \beta(x_{i-1})\,[\alpha(x_i)-\alpha(x_{i-1})].
\]
Then for each subinterval,
\[
\alpha(x_i)\beta(x_i)-\alpha(x_{i-1})\beta(x_{i-1})
=\alpha(x_i)[\beta(x_i)-\beta(x_{i-1})]+\beta(x_{i-1})[\alpha(x_i)-\alpha(x_{i-1})].
\]
Summing over $i=1,\dots,n$ gives
\[
\alpha(b)\beta(b)-\alpha(a)\beta(a)
=\sum_{i=1}^n \alpha(x_i)[\beta(x_i)-\beta(x_{i-1})]
+\sum_{i=1}^n \beta(x_{i-1})[\alpha(x_i)-\alpha(x_{i-1})].
\tag{2}
\]
That is,
\[
S_R(\alpha,\beta;P)+S_L(\beta,\alpha;P)=\alpha(b)\beta(b)-\alpha(a)\beta(a).
\]

Since $\alpha\in R(\beta)$, the upper and lower sums for $\alpha$ with respect to $\beta$ can be made arbitrarily close to the unique integral value 
\[
I=\int_a^b \alpha\,d\beta.
\]
In particular, (1) implies that for every $\epsilon>0$, there is a partition $P$ with
\[
\sum_{i=1}^n\Bigl[\alpha(x_i)-\alpha(x_{i-1})\Bigr]\Bigl[\beta(x_i)-\beta(x_{i-1})\Bigr]<\epsilon.
\]
Because $\alpha$ is increasing, any choice of sample points in the definition of the Riemann--Stieltjes sum will differ from the specific choices in $S_R(\alpha,\beta;P)$ and $S_L(\beta,\alpha;P)$ by no more than the sum above. Hence, for every $\epsilon>0$, the quantity
\[
\Bigl|\,S_R(\alpha,\beta;P)+S(\beta,\alpha;P)-\Bigl[\alpha(b)\beta(b)-\alpha(a)\beta(a)\Bigr]\,\Bigr|
\]
is bounded by the same sum and can be made less than $\epsilon$.

Since $S_R(\alpha,\beta;P)$ converges to $I$, equation (2) forces the Riemann--Stieltjes sums for $\beta$ with respect to $\alpha$ to converge to the unique number
\[
\alpha(b)\beta(b)-\alpha(a)\beta(a)-I.
\]
Thus, by definition, $\beta\in R(\alpha)$.

By symmetry, if $\beta\in R(\alpha)$ then $\alpha\in R(\beta)$. Therefore,
\[
\alpha\in R(\beta) \quad\Longleftrightarrow\quad \beta\in R(\alpha).
\]
\end{proof}
\begin{proof}
    (i) $\iff$ (iii). Because \(\alpha\) is increasing, the infimum on \([x_{i-1},x_i]\) is \(\alpha(x_{i-1})\) and the supremum is \(\alpha(x_i)\); hence,
\[
U(\alpha,\beta,P)-L(\alpha,\beta,P)=\sum_{i=1}^n \Bigl[\alpha(x_i)-\alpha(x_{i-1})\Bigr]\Bigl[\beta(x_i)-\beta(x_{i-1})\Bigr].
\]
By definition, \(\alpha\in R(\beta)\) if and only if for every \(\varepsilon>0\) there exists a partition \(P\) such that
\[
U(\alpha,\beta,P)-L(\alpha,\beta,P) < \varepsilon.
\]
Thus, (i) holds if and only if for every \(\varepsilon>0\) there exists a partition \(P\) with
\[
\sum_{i=1}^n \Bigl[\alpha(x_i)-\alpha(x_{i-1})\Bigr]\Bigl[\beta(x_i)-\beta(x_{i-1})\Bigr] < \varepsilon,
\]
which is exactly statement (iii).
\end{proof}
\end{exercise}

\begin{exercise}{4}
    \begin{proof}
Since $\alpha$ is continuous and non-decreasing on $[a,b]$, its image covers the entire interval 
\(\bigl[\alpha(a),\,\alpha(b)\bigr]\).  For each $y$ in that range, define 
\[
\alpha^{-1}(y)
\;:=\;
\inf\{\,x \in [a,b]\;:\;\alpha(x)\,\ge\,y\}.
\]
Because $\alpha$ is non-decreasing and continuous, the set $\{x : \alpha(x) \ge y\}$ is nonempty whenever $y \le \alpha(b)$, so the infimum is well-defined. Also, we actually get $\alpha\bigl(\alpha^{-1}(y)\bigr) = y$ for each $y \in [\alpha(a), \alpha(b)]$.  We can check this by: $\alpha\bigl(\alpha^{-1}(y)\bigr)\ge y$ by definition, and if it were 
   $> y$, continuity would force values $> y$ to appear just below $\alpha^{-1}(y)$ as well, contradicting 
   minimality of that infimum.  Thus equality holds.
Define
\[
y_k
\;:=\;
\alpha(a)\;+\;k\,\frac{\alpha(b)-\alpha(a)}{n}
\quad
\text{for }k=0,1,\dots,n.
\]

Hence
\[
\alpha(a) = y_0 < y_1 < \dots < y_n = \alpha(b),
\]

and each gap $y_k - y_{k-1}$ is exactly $\frac{\alpha(b)-\alpha(a)}{n}$.

We then define
\[
x_k \;:=\; \alpha^{-1}(y_k),
\quad
k=0,1,\dots,n.
\]
Since $\alpha^{-1}$ is non-decreasing, we get 
\[
a = x_0 \;\le\; x_1 \;\le\; \dots \;\le\; x_n = b.
\]
By construction, 
\(\alpha(x_k) = y_k\) for each $k$.  Therefore
\[
\alpha(x_k) - \alpha(x_{k-1})
\;=\;
y_k - y_{k-1}
\;=\;
\frac{\alpha(b) - \alpha(a)}{n}.
\]
In other words, all the increments in $\alpha$ along the partition 
\(\{x_0,\dots,x_n\}\) are exactly the same.

Thus, by definition,
\[
\Delta \alpha(P_n)
\;=\;
\max_{1 \le k \le n}\bigl[\alpha(x_k) - \alpha(x_{k-1})\bigr]
\;=\;
\frac{\alpha(b) - \alpha(a)}{n}.
\]
Hence we have constructed the required partition $P_n$.
\end{proof}
\end{exercise}

\begin{exercise}{5}
    \begin{proof}
Since $f$ is bounded and $f([a,b])\subseteq [c,d]$, we know that $\phi\bigl(f(x)\bigr)$ is wellâ€defined for all $x\in [a,b]$. 
Because $\phi$ is continuous on the compact set $[c,d]$, it is uniformly continuous there. 
Hence for every $\varepsilon>0$, there exists a $\delta>0$ such that
\[
y_1,y_2 \in [c,d]
\quad\text{and}\quad
|y_1-y_2| < \delta
\quad\Longrightarrow\quad
|\phi(y_1)-\phi(y_2)| < \varepsilon.
\tag{1}
\]

Because $f \in R(\alpha)$, for every $\eta>0$ there exists a partition $P$ such that 
\[
U(f,\alpha,P) - L(f,\alpha,P) \;<\;\eta,
\]
where
\[
U(f,\alpha,P) - L(f,\alpha,P)
\;=\;
\sum_{i=1}^n (M_i - m_i)\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr].
\tag{2}
\]

We want to show $\phi\circ f \in R(\alpha)$ by verifying the analogous criterion.  Define
\[
U(\phi\circ f,\alpha,P) 
= \sum_{i=1}^n 
\sup_{x\in [x_{i-1},x_i]}\phi\bigl(f(x)\bigr)\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr],
\]
\[
L(\phi\circ f,\alpha,P)
= \sum_{i=1}^n 
\inf_{x\in [x_{i-1},x_i]}\phi\bigl(f(x)\bigr)\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr].
\]
Let 
\[
\widetilde{M}_i 
\;=\; \sup_{y\in [m_i,M_i]}\phi(y),
\quad
\widetilde{m}_i
\;=\; \inf_{y\in [m_i,M_i]}\phi(y).
\]
Because $f(x)$ for $x\in[x_{i-1},x_i]$ lies in $[m_i,M_i]$, we see
\[
\sup_{x\in[x_{i-1},x_i]} \phi\bigl(f(x)\bigr)
\;=\; \widetilde{M}_i,
\quad
\inf_{x\in[x_{i-1},x_i]} \phi\bigl(f(x)\bigr)
\;=\; \widetilde{m}_i.
\]
Thus
\[
U(\phi\circ f,\alpha,P) 
- 
L(\phi\circ f,\alpha,P)
\;=\;
\sum_{i=1}^n \bigl[\widetilde{M}_i - \widetilde{m}_i\bigr]\,
\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr].
\tag{3}
\]

Because $\phi$ is uniformly continuous on $[c,d]$, we have \((1)\): for any $\varepsilon>0$, there is a $\delta>0$ such that $|y_1-y_2|<\delta$ implies $|\phi(y_1)-\phi(y_2)|<\varepsilon$.

For each $i$, the interval $[m_i,M_i]$ has length $M_i - m_i$.  If $M_i - m_i<\delta$, then 
\[
\widetilde{M}_i - \widetilde{m}_i
\;=\;
\sup_{y\in[m_i,M_i]}\phi(y) 
\;-\;
\inf_{y\in[m_i,M_i]}\phi(y)
\;\le\;
\varepsilon
\]
by the uniform continuity of $\phi$.  In other words, whenever $M_i - m_i<\delta$, we get $\widetilde{M}_i - \widetilde{m}_i \le \varepsilon$.
Given any $\varepsilon>0$, choose $\delta>0$ as above.  Now, we know $f\in R(\alpha)$, so from $(2)$ there is a partition $P$ such that
\[
\sum_{i=1}^n (M_i - m_i)\,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr] 
\;<\;
\delta \,\bigl[\alpha(b)-\alpha(a)\bigr]
\]
(if $\alpha$ is increasing; or more generally, we can make the sum of $(M_i - m_i)\Delta\alpha_i$ as small as needed).  In particular, each term $(M_i - m_i)\Delta\alpha_i$ is small; in fact, if any $(M_i - m_i)$ were $\ge\delta$, the product would be $\ge \delta\,\Delta\alpha_i$.  By choosing $P$ finely enough, we can ensure $M_i - m_i<\delta$ for all $i$.  Consequently, $\widetilde{M}_i - \widetilde{m}_i \le \varepsilon$ for all $i$. Hence from $(3)$:
\[
U(\phi\circ f,\alpha,P) - L(\phi\circ f,\alpha,P)
\;=\;
\sum_{i=1}^n \bigl[\widetilde{M}_i - \widetilde{m}_i\bigr]\,
\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr]
\;\le\;
\sum_{i=1}^n \varepsilon \,\bigl[\alpha(x_i)-\alpha(x_{i-1})\bigr]
\;=\;
\varepsilon\,\bigl[\alpha(b)-\alpha(a)\bigr].
\]
Since $\alpha(b)-\alpha(a)$ is fixed, for any given $\epsilon'>0$, we can choose $\varepsilon$ so that $\varepsilon[\alpha(b)-\alpha(a)]<\epsilon'$, and make $U(\phi\circ f,\alpha,P)-L(\phi\circ f,\alpha,P)<\epsilon'$.  This shows that $\phi\circ f\in R(\alpha)$.
\end{proof}
\end{exercise}

\end{document}